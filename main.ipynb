{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import split_data, add_subcomments, create_comments_list\n",
    "from tqdm import tqdm\n",
    "from relation_classifier import relation_classifier\n",
    "from model_summ import T5summarizer\n",
    "from textrank import preprocess_comment\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5summarizer(\"gromoboy/rut5_base_summ_brand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "dataset_input = \"dataset.jsonl\"\n",
    "dataset_output = \"result.jsonl\"\n",
    "split_type = \"all_comments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL\n",
    "BATCH_SIZE = 4 # model batch size\n",
    "CHUNK_SIZE = 30 # amount of sentences which we send for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "443898it [00:31, 14043.07it/s]\n",
      "Root ID sort comments: 100%|██████████| 433890/433890 [00:00<00:00, 1868060.66it/s]\n",
      "Gathering preprocessed posts/comments together: 100%|██████████| 10008/10008 [00:01<00:00, 9291.75it/s]\n"
     ]
    }
   ],
   "source": [
    "jsonl_file_path = f'data/{dataset_input}'\n",
    "posts, comments = split_data(jsonl_file_path)\n",
    "\n",
    "comments_by_root = {}\n",
    "for comment in tqdm(comments, desc=\"Root ID sort comments\"):\n",
    "    root_id = comment[\"root_id\"]\n",
    "\n",
    "    if root_id in comments_by_root:\n",
    "        comments_by_root[root_id].append(comment)\n",
    "    else:\n",
    "        comments_by_root[root_id] = [comment]\n",
    "\n",
    "\n",
    "data = []\n",
    "for index, post in enumerate(tqdm(posts, desc=\"Gathering preprocessed posts/comments together\")):\n",
    "\n",
    "    comments_structured = add_subcomments(comments_by_root[post[\"id\"]])\n",
    "    comments_unstructured = create_comments_list(comments_structured)\n",
    "\n",
    "    data.append([(post['hash'], post['text']),\n",
    "                 comments_unstructured])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Your max_length is set to 200, but your input_length is only 109. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=54)\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Your max_length is set to 200, but your input_length is only 161. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=80)\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.38s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Your max_length is set to 200, but your input_length is only 132. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=66)\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.24s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Your max_length is set to 200, but your input_length is only 195. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=97)\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.84s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Your max_length is set to 200, but your input_length is only 90. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.69s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Your max_length is set to 200, but your input_length is only 83. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Your max_length is set to 200, but your input_length is only 65. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.66s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Your max_length is set to 200, but your input_length is only 114. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=57)\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.49s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Your max_length is set to 200, but your input_length is only 194. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=97)\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.69s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Your max_length is set to 200, but your input_length is only 147. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=73)\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for post_data in data[2020:2040]:\n",
    "    post, comments = post_data\n",
    "\n",
    "    post_hash = post[0]\n",
    "\n",
    "    if split_type == \"post_comments\":\n",
    "        comments = relation_classifier(post, comments, \"direct\")\n",
    "    \n",
    "    if split_type == \"topic_comments\":\n",
    "        comments = relation_classifier(post, comments, \"indirect\")\n",
    "\n",
    "    comments_texts, comments_hash = [], []\n",
    "    for comment in comments:\n",
    "        hash_, text = comment\n",
    "        comments_texts.append(preprocess_comment(text))\n",
    "        comments_hash.append(hash_)\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(comments_texts), CHUNK_SIZE):\n",
    "        chunks.append(\"\\n\".join(comments_texts[i : i+CHUNK_SIZE]))\n",
    "    \n",
    "    #print(\"==\", len(chunks))\n",
    "    if len(chunks) != 0:\n",
    "        summary = model.batch_summarize(chunks, BATCH_SIZE)\n",
    "    else:\n",
    "        summary = \"Отсутствует содержание\"\n",
    "    \n",
    "    result.append({\"summary\": summary,\n",
    "                    \"post_hash\": post_hash,\n",
    "                    \"comments_hash\": comments_hash})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"data/{dataset_output}\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(filename, 'w', encoding='utf-8') as file:\n",
    "    for entry in result:\n",
    "        # Convert the dictionary to a JSON string and write to the file\n",
    "        json_record = json.dumps(entry, ensure_ascii=False)\n",
    "        file.write(json_record + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relation_classifier(post, comments, \"indirect\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_hash = [comment[0] for comment in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relation_classifier(post, comments, \"indirect\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
